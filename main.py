__author__ = 'Ivan'

import matplotlib.pyplot as plt
import numpy as np

import graphUtils as gutil
from dataAssimilation.reducedLinearModel import reducedLinearModel as romClass
from ioUtils import ioUtils


def cls(): print("\n" * 100)

sizeEnsemble = 100
baseFolder = '.\\..\\..\\Writings\\paper_enmor\\matlab\\raw_data\\'


# ===================================
# - Upload the model runs
# -----------------------------------

# Process the model ensemble data
# modelEnsemble = ioUtils.csv2array(baseFolder + '20141009_results.txt',delimiter=' ')
modelEnsemble = ioUtils.csv2array(baseFolder + '20141101_results.txt', delimiter=' ')  #Load data
modelEnsemble = np.array(modelEnsemble)
xShoreCoord   = modelEnsemble[:, 0].reshape([20, 11, int(modelEnsemble.shape[0]/20/11)], order = 'F').copy()
xShoreCoord   = np.squeeze(xShoreCoord[:,0,0])
modelEnsemble = 1000*modelEnsemble[:, 1].reshape([20, 11, int(modelEnsemble.shape[0]/20/11)], order = 'F')

# process the input data
# inputVector = ioUtils.csv2array(baseFolder + '20141009_parameters.txt', delimiter='\n')
inputVector = ioUtils.csv2array(baseFolder + '20141101_parameters.txt', delimiter='\n')
inputVector = np.array(inputVector)
inputVector = inputVector[:, 0].reshape([20, 11, int(inputVector.shape[0]/20/11)], order='F')
# ===================================


# ===================================
# - Get rid of faulty model runs
# -----------------------------------
modelErrors = np.unique(np.array(np.where(np.isnan(modelEnsemble))[2], dtype=int))

modelEnsemble = np.delete(modelEnsemble, modelErrors, 2)
modelEnsemble = np.delete(np.delete(modelEnsemble, 0, 0), 18, 0)
xShoreCoord   = np.delete(np.delete(xShoreCoord, 0, 0), 18, 0)

inputVector = np.delete(inputVector, modelErrors, 2)
inputVector = np.delete(inputVector, range(9, 20, 1), 0)
inputVector = np.delete(inputVector, range(1, 20, 1), 1)
inputVector = np.squeeze(inputVector, axis=1)
# ===================================


# ===================================
# - Get reference and observations
# -----------------------------------

# Choose a reference run that will be assumed to be the initial guess.
referenceSimulation = modelEnsemble[:, :, 0].copy()
referenceInput      = inputVector[:, [0]].copy()
modelEnsemble       = np.delete(modelEnsemble, 0, 2)
inputVector         = np.delete(inputVector, 0, 1)

# Choose a set of observations: ** Notice
observation_indices = range(25,1000,25)
observationSet      = modelEnsemble[:, :, observation_indices].copy()
observationInput    = inputVector[:, observation_indices].copy()
observationOperator = np.eye(referenceSimulation.shape[0])
# ===================================

# ===================================
# - Select one observation model and define its error properties
# -----------------------------------

# Which observation set do you want to use?
#iset = 30
iset = 37


initialStateCov = np.diag(np.var(modelEnsemble[:, 0, :], axis=1))
inputCov        = np.diag(np.var(inputVector, axis=1))

observationCov = np.ndarray([observationSet.shape[0], observationSet.shape[0],
                             observationSet.shape[1]])

# obsErr          = 0.01 # 2% of the observed value
# obsStd = obsErr * observationSet[:, :, iset]
obsStd = np.ones(observationSet[:, :, iset].shape)*0.05

for it in range(0, observationSet.shape[1]):
    observationCov[:,:,it] = np.diag(obsStd[:,it]*obsStd[:,it])

observationSet[:,:,iset] = observationSet[:,:,iset] + \
                           obsStd * np.random.randn( observationSet.shape[0],
                                                   observationSet.shape[1])

# ===================================
# - Build the reduced order model
# -----------------------------------

# Keep only the number of ensemble members desired
modelEnsemble = modelEnsemble[:,:,range(0,sizeEnsemble,1)]
inputVector   = inputVector[:,range(0,sizeEnsemble,1)]

'''
 - maxiter (int): Maximum number of iterations.
 - factr (float): The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps, where eps is
                          the machine precision, which is automatically generated by the code. Typical values for factr
                          are:1e12 for low accuracy; 1e7 for moderate accuracy; 10.0 for extremely high accuracy.
 - pgtol (float): The iteration will stop when max{|proj g_i | i = 1, ..., n} <= pgtol where pg_i is the i-th
                       component of the projected gradient.
'''
rom = romClass('Profile model', 'Idealized model', inputVector, modelEnsemble, referenceInput, referenceSimulation, \
               "percent", 0.9, maxiter=1, factr=1e-9, pgtol=1e-12, verbose=1)


# ===============================
# Assimilate observations
[analyzedState, analyzedInput, analysisCost] = rom.assimilateData( observationSet[:,:,iset], observationOperator, \
                                                                   observationCov, referenceInput, \
                                                                   referenceSimulation[:,[0]], inputCov, \
                                                                   initialStateCov)


a = np.concatenate((analyzedState,analyzedInput),axis=0)
e = np.concatenate((modelEnsemble[:,0,:],inputVector),axis=0)
d = e - np.repeat(a,e.shape[1],axis=1)
n = [np.linalg.norm(d[:,[i]]) for i in range(0,d.shape[1])]
ind = np.argmin(n)


gutil.graphicAnalysis(modelEnsemble, observationSet[:,:,iset],
                      observationInput[:,[iset]], referenceSimulation, referenceInput)

gutil.graphicResultAnalysis(analyzedState, analyzedInput, observationSet[:,:,iset], observationInput[:, [iset]], referenceSimulation, referenceInput, rom)
# gutil.romPerformacePcolor(modelEnsemble, inputVector, rom)
# gutil.romErrorAnalysis(rom, modelEnsemble, inputVector, xShoreCoord)
plt.show()

# t=2; iset = 2; ind = np.arange(modelEnsemble.shape[0]); cheat = rom.evaluateROM(modelEnsemble[:,[0],iset],inputVector[:,[iset]]); plt.plot(ind,cheat[:,[t]]); plt.plot(ind,modelEnsemble[:,[t],iset],color='r'); plt.show()
# t=2; iset = 2; ind = np.arange(modelEnsemble.shape[0]); cheat = rom.evaluateROM(modelEnsemble[:,[0],iset],inputVector[:,[iset]]); plt.plot(ind,cheat[:,[t]]); plt.plot(ind,cheat[:,[t-1]],color='r'); plt.show()

'''
numStates = referenceSimulation.shape[0]
invInputCovariance = np.linalg.inv(inputCov)
invStateCovariance = np.linalg.inv(initialStateCov)
invObsCovariance = np.ndarray([numStates, numStates, observationSet.shape[1]])
for iobs in range(0, observationCov.shape[2]):
    invObsCovariance[:, :, iobs] = np.linalg.inv(observationCov[:, :, iobs])

optimArg1 = np.zeros([referenceSimulation.shape[0] + referenceInput.shape[0], 1])
optimArg1[0:referenceSimulation.shape[0], [0]] = analyzedState
optimArg1[referenceSimulation.shape[0]:, [0]] = analyzedInput

optimArg = np.zeros([referenceSimulation.shape[0] + referenceInput.shape[0], 1])
optimArg[0:referenceSimulation.shape[0], [0]] = referenceSimulation[:,[0]]
optimArg[referenceSimulation.shape[0]:, [0]] = referenceInput

rom.assimilationCost(optimArg, observationSet[:,:,iset], observationOperator, invObsCovariance, invStateCovariance, invInputCovariance)
rom.assimilationCost(optimArg1, observationSet[:,:,iset], observationOperator, invObsCovariance, invStateCovariance, invInputCovariance)
'''